---
title: "Community Detection with Dirichlet Process Prior"
author: "Davis Berlind"
date: "2022-06-10"
output:
  html_document:
    mathjax: "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_CHTML"
math: true
header-includes:
  - \usepackage{amsmath}
  - \usepackage{amsfonts}
  - \usepackage{amssymb}
  - \usepackage{enumitem}
  - \usepackage{bbm}
  - \usepackage{graphicx}
  - \usepackage[ruled,vlined,linesnumbered]{algorithm2e}
  - \counterwithin*{equation}{section}
  - \newcommand{\sforall}{\;\forall\;}
  - \newcommand{\E}{\mathbb{E}}
  - \newcommand{\Var}{\text{Var}}
  - \newcommand{\Cov}{\text{Cov}}
  - \newcommand{\p}{\text{P}}
  - \newtheorem{lemma}{Lemma}
  - \newtheorem{theorem}{Theorem}
bibliography: monk-references.bib
---

## Background

In his 1968 PhD thesis at Cornell University, Samuel F. Sampson conducted an ethnographic study of the community structure in a New England monastery. The monks in the study were asked to rank which of their peers they liked most and which they liked least, listing a first, second, and third choice. This survey was performed on five separate occasions over a 12-month period. Shortly after the fourth survey was administered, four members of the monastery were expelled. Sampson was interested in identifying the social dynamics that led to the expulsions. He used his survey to classify the novice monks into three categories: Young Turks, Loyal Opposition, Outcasts. In the [data set](https://networkdata.ics.uci.edu/netdata/html/sampson.html), any monk that ranked a colleague in their top three positive relations in any survey was considered to 'like' that monk, resulting in an adjacency matrix $\mathbf{Y}\in\{0,1\}^{18\times 18}$ with $Y_{ij}=1$ indicating a one-way 'liking' relation from monk $i$ to monk $j$. 

@handcock07 developed a latent-position clustering model to detect communities within the network of monks. Rather than treat the number of communities as a random variable, @handcock07 selected the number of communities based on the BIC. In this post I will lay out a fully Bayesian nonparametric approach to community detection by placing a Dirichlet process prior on the number of communities.

## Model

Given a graph $\mathcal{G}$, we assume that if nodes $i$ and $j$ are in the same community, they are more likely to be connected by an edge in $\mathcal{G}$. @hoff2002latent generalized this idea to a latent space model for network data. They assume each node has a position $Z_i \in \mathbb{R}^d$ in a $d$-dimensional latent social space. The advantage of using latent positions is that it induces transitivity, so that if node $i$ is close to node $j$ in the latent space, and node $j$ is close to node $k$, then we will also have that node $i$ and $k$ are close therefore more likely to be connected in $\mathcal{G}$. Formally, if we have an adjacency matrix $\mathbf{Y}$, where $Y_{ij} = 1$ if there is (directed) edge going from node $i$ to $j$, and $Y_{ij} = 0$ otherwise, then we have  
\begin{gather*}
    p(\mathbf{Y} \;|\; \mathbf{Z}, \beta) = \prod_{i \neq j} p(Y_{ij} \;|\; Z_i, Z_j), \\
    Y_{ij} \;|\; Z_i, Z_j, \beta \overset{\text{ind.}}{\sim} \text{Bernoulli}(\pi_{ij}), \\ 
    \pi_{ij} = \text{Logistic}(\beta - \lVert Z_i - Z_j \rVert_2).
\end{gather*}
@handcock07 showed that it is possible to include edge specific covariates, $X_{ij}$. For simplicity, I just choose to include an intercept $\beta$, and focus on modeling the latent positions, $\mathbf{Z}$.

### Dirchlet Process Prior

To model the latent social space, @handcock07 fixed some $K \in \mathbb{N}$ and adopted a Gaussian mixture model,
\begin{align*}
    X_{ik} &\overset{\text{ind.}}{\sim} \mathcal{N}_d(\mu_k, \sigma_k^2 \mathbf{I}_d), \\
    Z_i &= \sum_{k=1}^K \lambda_k X_{ik}, \\
    \lambda &\sim \text{Dirichlet}(\nu).
\end{align*}
@handcock07 fix the number of clusters *a priori* and find the $K$ that maximizes integrated likelihood (approximated by the BIC). We can circumvent this model selection problem with a Bayesian nonparametric approach. Note that a $K$ cluster mixture model is just a $K+1$ mixture model with the restriction that $\lambda_{k+1} = 0$. In this sense, every size $K' < K$ mixture model is embedded in the $K$ cluster mixture model. If we let $K \to \infty$ and can find a suitable prior for $\{\lambda_k\}_{k\geq 1}$, then our model will account for every possible configuration of $K$. Then, by treating the number of mixture components as a random variable, we can correctly make probabilistic statements about the number of clusters, including finding the *a posteriori* most likely number of clusters.

To implement an infinite mixture model, we can adopt a Dirichlet process (DP) prior for the distribution that generates the latent positions $Z_i$. The Dirichlet process is a distribution over distributions that was first formalized by @ferguson. @AntoniakCharlesE.1974MoDP showed the DP can be used as prior over distributions in mixture models. Formally, let $(\Theta, \mathcal{F}, G_0)$ be some measurable space and let $\{A_i\}_{i=1}^r$ be a finite measurable partition of $\Theta$. Then for another measure on this space $G$, we say that $G \sim \text{DP}(\alpha, G_0)$ if for some $\alpha > 0$ we have $$\{G(A_i)\}_{i=1}^r \sim \text{Dirichlet}(\{G_0(A_i)\}_{i=1}^r).$$ Note that for some measurable $A \subset \mathcal{F}$ we have 
\begin{align*}
    (G(A), G(A^c)) \sim \text{Dirichlet}(\alpha G_0(A), \alpha G_0(A^c)), 
\end{align*}
and thus
\begin{align*}
    \E_{\alpha,G_0}[G(A)] &= \frac{\alpha G_0(A)}{\alpha G_0(A) + \alpha G_0(A^c)} = G_0(A) \tag{$G_0(A) + G_0(A^c) = 1$} \\
    \Var_{\alpha,G_0}(G(A)) &= \frac{G_0(A)[1 - G_0(A)]}{1 + \alpha}.
\end{align*}
So $G_0$, can be thought of as the mean distribution and $\alpha$ controls the strength of concentration around this mean with $G(A) \overset{L_2}{\longrightarrow} G_0(A)$ for any measurable set $A$ as $\alpha \to \infty$. Rewriting the model of @handcock07 to include a DP prior, we have
\begin{align*}
    Z_{i} \;|\; \mu_i, \sigma_i^2 &\sim \mathcal{N}_d(\mu_i, \sigma_i^2\mathbf{I}_d), \\
    (\mu_i, \sigma_i) \;|\; G &\sim G, \\
    G &\sim \text{DP}(\alpha, \mathcal{N}_d(0,\omega^2\mathbf{I}_d) \times \text{Inverse Gamma}(a,b)).
\end{align*}
We can adopt the same prior assumptions as @handcock07 by letting $a = 1$, $b = 0.103 / 2$, and $\omega^2 = 2$. 

## Acknowledgment

This post was adapted from a project for STATS 202C at UCLA that I worked on with my colleagues Sophie Phillips and John Baierl. Thanks to Sophie and John for their help!

## References


